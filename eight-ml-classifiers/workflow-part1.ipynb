{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Report - Part 1\n",
    "\n",
    "This script generates eight best machine learning models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Import Libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "import pandas as pd\n",
    "# data preprocessing\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "# feature extraction\n",
    "from keras.utils import np_utils\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "# machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import (AdaBoostClassifier, RandomForestClassifier,\n",
    "                              VotingClassifier)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.metrics.classification import log_loss\n",
    "from sklearn.model_selection import (GridSearchCV, StratifiedKFold,\n",
    "                                     learning_curve, train_test_split)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "source": [
    "## Import Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/dataset/training_variants.zip\")\n",
    "print(\"Number of data points : \", data.shape[0])\n",
    "print(\"Number of features : \", data.shape[1])\n",
    "print(\"Features : \", data.columns.values)\n",
    "\n",
    "data_text = pd.read_csv(\n",
    "    \"../data/dataset/training_text.zip\",\n",
    "    sep=\"\\|\\|\",\n",
    "    engine=\"python\",\n",
    "    names=[\"ID\", \"TEXT\"],\n",
    "    skiprows=1,\n",
    ")\n",
    "print(\"Number of data points : \", data_text.shape[0])\n",
    "print(\"Number of features : \", data_text.shape[1])\n",
    "print(\"Features : \", data_text.columns.values)"
   ]
  },
  {
   "source": [
    "## Data Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"\\w+'?\\w+|\\w+\")\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words = set(stop_words).union(STOP_WORDS)\n",
    "nlp = spacy.load(\"en\", disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "def make_token(x):\n",
    "    \"\"\" Tokenize the text (remove punctuations and spaces)\"\"\"\n",
    "    return tokenizer.tokenize(str(x))\n",
    "def remove_stopwords(x):\n",
    "    return [token for token in x if token not in final_stop_words]\n",
    "def lemmatization(x):\n",
    "    lemma_result = []\n",
    "    for words in x:\n",
    "        doc = nlp(words)\n",
    "        for token in doc:\n",
    "            lemma_result.append(token.lemma_)\n",
    "    return lemma_result\n",
    "def pipeline(total_text, index, column):\n",
    "    \"\"\" A pipeline to process text data \"\"\"\n",
    "    if type(total_text) is str:\n",
    "        total_text = total_text.lower()\n",
    "        total_text = make_token(total_text)\n",
    "        total_text = remove_stopwords(total_text)\n",
    "        total_text = lemmatization(total_text)\n",
    "        string = \" \".join(total_text)\n",
    "        data_text[column][index] = string\n",
    "for index, row in data_text.iterrows():\n",
    "    if type(row[\"TEXT\"]) is str:\n",
    "        pipeline(row[\"TEXT\"], index, \"TEXT\")\n",
    "    else:\n",
    "        print(\"there is no text description for id:\", index)\n",
    "\n",
    "### merge genes, variations and text data by ID\n",
    "result = pd.merge(data, data_text, on=\"ID\", how=\"left\")\n",
    "result.loc[result[\"TEXT\"].isnull(), \"TEXT\"] = result[\"Gene\"] + \" \" + result[\"Variation\"]\n",
    "result.Gene = result.Gene.str.replace(\"\\s+\", \"_\")\n",
    "result.Variation = result.Variation.str.replace(\"\\s+\", \"_\")\n",
    "\n",
    "## write to pickle\n",
    "pd.to_pickle(result, \"result_non_split.pkl\")"
   ]
  },
  {
   "source": [
    "## Feature Extraction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxFeats = 10000\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    min_df=5,\n",
    "    max_features=maxFeats,\n",
    "    ngram_range=(1, 2),\n",
    "    analyzer=\"word\",\n",
    "    stop_words=\"english\",\n",
    "    token_pattern=r\"\\w+\",\n",
    ")\n",
    "tfidf.fit(result[\"TEXT\"])\n",
    "\n",
    "cvec = CountVectorizer(\n",
    "    min_df=5,\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=maxFeats,\n",
    "    analyzer=\"word\",\n",
    "    stop_words=\"english\",\n",
    "    token_pattern=r\"\\w+\",\n",
    ")\n",
    "cvec.fit(result[\"TEXT\"])\n",
    "\n",
    "# try n_components between 360-390\n",
    "svdT = TruncatedSVD(n_components=390, n_iter=5)\n",
    "svdTFit = svdT.fit_transform(tfidf.transform(result[\"TEXT\"]))\n",
    "\n",
    "\n",
    "def buildFeatures(df):\n",
    "    \"\"\"This is a function to extract features, df argument should be\n",
    "    a pandas dataframe with only Gene, Variation, and TEXT columns\"\"\"\n",
    "\n",
    "    temp = df.copy()\n",
    "\n",
    "    print(\"Encoding...\")\n",
    "    temp = pd.get_dummies(temp, columns=[\"Gene\", \"Variation\"], drop_first=True)\n",
    "\n",
    "    print(\"TFIDF...\")\n",
    "    temp_tfidf = tfidf.transform(temp[\"TEXT\"])\n",
    "\n",
    "    print(\"Count Vecs...\")\n",
    "    temp_cvec = cvec.transform(temp[\"TEXT\"])\n",
    "\n",
    "    print(\"Latent Semantic Analysis Cols...\")\n",
    "    del temp[\"TEXT\"]\n",
    "\n",
    "    tempc = list(temp.columns)\n",
    "\n",
    "    temp_lsa_tfidf = svdT.transform(temp_tfidf)\n",
    "    temp_lsa_cvec = svdT.transform(temp_cvec)\n",
    "\n",
    "    for i in range(np.shape(temp_lsa_tfidf)[1]):\n",
    "        tempc.append(\"lsa_t\" + str(i + 1))\n",
    "    for i in range(np.shape(temp_lsa_cvec)[1]):\n",
    "        tempc.append(\"lsa_c\" + str(i + 1))\n",
    "    temp = pd.concat(\n",
    "        [\n",
    "            temp,\n",
    "            pd.DataFrame(temp_lsa_tfidf, index=temp.index),\n",
    "            pd.DataFrame(temp_lsa_cvec, index=temp.index),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    return temp, tempc\n",
    "\n",
    "\n",
    "trainDf, traincol = buildFeatures(result[[\"Gene\", \"Variation\", \"TEXT\"]])\n",
    "trainDf.columns = traincol\n",
    "pd.to_pickle(trainDf, \"trainDf.pkl\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Training Classifiers and Tuning Hyperparameter "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = pd.read_pickle(\"result_non_split.pkl\")\n",
    "labels = result.Class - 1\n",
    "# trainDf = pd.read_pickle(\"trainDf.pkl\")\n",
    "\n",
    "# for cross Validation\n",
    "kfold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "\n",
    "# split data into training data and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    trainDf, labels, test_size=0.2, random_state=5, stratify=labels\n",
    ")\n",
    "\n",
    "# encode labels\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "encoded_test_y = np_utils.to_categorical((le.inverse_transform(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Support Vector Machines\n",
    "svr = SVC(probability=True)\n",
    "# Hyperparameter tuning - Grid search cross validation\n",
    "svr_CV = GridSearchCV(\n",
    "    svr,\n",
    "    param_grid={\n",
    "        \"C\": [0.1, 1, 10, 100],\n",
    "        \"gamma\": [1, 0.1, 0.01, 0.001],\n",
    "        \"kernel\": [\"poly\", \"rbf\", \"sigmoid\", \"linear\"],\n",
    "        \"tol\": [1e-4],\n",
    "    },\n",
    "    cv=kfold,\n",
    "    verbose=False,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "svr_CV.fit(X_train, y_train)\n",
    "svr_CV_best = svr_CV.best_estimator_\n",
    "print(\"Best score: %0.3f\" % svr_CV.best_score_)\n",
    "print(\"Best parameters set:\", svr_CV.best_params_)\n",
    "\n",
    "# save the model\n",
    "filename = \"svr_CV_best.sav\"\n",
    "pickle.dump(svr_CV_best, open(filename, \"wb\"))\n",
    "## load the model\n",
    "# with (open(filename, \"rb\")) as openfile:\n",
    "# svr_CV_best = pickle.load(openfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. Logistic Regression\n",
    "logreg = LogisticRegression(multi_class=\"multinomial\")\n",
    "# Hyperparameter tuning - Grid search cross validation\n",
    "logreg_CV = GridSearchCV(\n",
    "    estimator=logreg,\n",
    "    param_grid={\"C\": np.logspace(-3, 3, 7), \"penalty\": [\"l1\", \"l2\"]},\n",
    "    cv=kfold,\n",
    "    verbose=False,\n",
    ")\n",
    "logreg_CV.fit(X_train, y_train)\n",
    "logreg_CV_best = logreg_CV.best_estimator_\n",
    "print(\"Best score: %0.3f\" % logreg_CV.best_score_)\n",
    "print(\"Best parameters set:\", logreg_CV.best_params_)\n",
    "\n",
    "# save the model\n",
    "filename = \"logreg_CV_best.sav\"\n",
    "pickle.dump(logreg_CV_best, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. k-Nearest Neighbors\n",
    "knn = KNeighborsClassifier()\n",
    "# Hyperparameter tuning - Grid search cross validation\n",
    "param_grid = {\"n_neighbors\": range(2, 10)}\n",
    "knn_CV = GridSearchCV(\n",
    "    estimator=knn, param_grid=param_grid, cv=kfold, verbose=False\n",
    ").fit(X_train, y_train)\n",
    "knn_CV_best = knn_CV.best_estimator_\n",
    "print(\"Best score: %0.3f\" % knn_CV.best_score_)\n",
    "print(\"Best parameters set:\", knn_CV.best_params_)\n",
    "\n",
    "# save the model\n",
    "filename = \"knn_CV_best.sav\"\n",
    "pickle.dump(knn_CV_best, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Random Forest\n",
    "random_forest = RandomForestClassifier()\n",
    "param_grid = {\n",
    "    \"bootstrap\": [True, False],\n",
    "    \"max_depth\": [5, 8, 10, 20, 40, 50, 60, 80, 100],\n",
    "    \"max_features\": [\"auto\", \"sqrt\"],\n",
    "    \"min_samples_leaf\": [1, 2, 4, 10, 20, 30, 40],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"n_estimators\": [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000],\n",
    "}\n",
    "random_forest_CV = GridSearchCV(\n",
    "    estimator=random_forest, param_grid=param_grid, cv=kfold, verbose=False, n_jobs=-1\n",
    ")\n",
    "random_forest_CV.fit(X_train, y_train)\n",
    "random_forest_CV_best = random_forest_CV.best_estimator_\n",
    "print(\"Best score: %0.3f\" % random_forest_CV.best_score_)\n",
    "print(\"Best parameters set:\", random_forest_CV.best_params_)\n",
    "\n",
    "# save the model\n",
    "filename = \"random_forest_CV_best.sav\"\n",
    "pickle.dump(random_forest_CV_best, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. Adaboost\n",
    "param_grid = {\n",
    "    \"base_estimator__max_depth\": [5, 10, 20, 50, 100, 150, 200],\n",
    "    \"n_estimators\": [100, 500, 1000, 1500, 2000],\n",
    "    \"learning_rate\": [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "    \"algorithm\": [\"SAMME\", \"SAMME.R\"],\n",
    "}\n",
    "Ada_Boost = AdaBoostClassifier(DecisionTreeClassifier())\n",
    "Ada_Boost_CV = GridSearchCV(\n",
    "    estimator=Ada_Boost, param_grid=param_grid, cv=kfold, verbose=False, n_jobs=10\n",
    ")\n",
    "Ada_Boost_CV.fit(X_train, y_train)\n",
    "Ada_Boost_CV_best = Ada_Boost_CV.best_estimator_\n",
    "print(\"Best score: %0.3f\" % Ada_Boost_CV.best_score_)\n",
    "print(\"Best parameters set:\", Ada_Boost_CV.best_params_)\n",
    "\n",
    "# save the model\n",
    "filename = \"Ada_Boost_CV_best.sav\"\n",
    "pickle.dump(Ada_Boost_CV_best, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6. XGBoost\n",
    "xgb_clf = xgb.XGBClassifier(objective=\"multi:softprob\")\n",
    "parameters = {\n",
    "    \"n_estimators\": [200, 300, 400],\n",
    "    \"learning_rate\": [0.001, 0.003, 0.005, 0.006, 0.01],\n",
    "    \"max_depth\": [4, 5, 6],\n",
    "}\n",
    "xgb_clf_cv = GridSearchCV(\n",
    "    estimator=xgb_clf, param_grid=parameters, n_jobs=-1, cv=kfold\n",
    ").fit(X_train, y_train)\n",
    "xgb_clf_cv_best = xgb_clf_cv.best_estimator_\n",
    "print(\"Best score: %0.3f\" % xgb_clf_cv.best_score_)\n",
    "print(\"Best parameters set:\", xgb_clf_cv.best_params_)\n",
    "\n",
    "# Save the model\n",
    "filename = \"xgb_clf_cv_best.sav\"\n",
    "pickle.dump(xgb_clf_cv_best, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7. MLPClassifier\n",
    "mlp = MLPClassifier()\n",
    "param_grid = {\n",
    "    \"hidden_layer_sizes\": [i for i in range(5, 25, 5)],\n",
    "    \"solver\": [\"sgd\", \"adam\", \"lbfgs\"],\n",
    "    \"learning_rate\": [\"constant\", \"adaptive\"],\n",
    "    \"max_iter\": [500, 1000, 1200, 1400, 1600, 1800, 2000],\n",
    "    \"alpha\": [10.0 ** (-i) for i in range(-3, 6)],\n",
    "    \"activation\": [\"tanh\", \"relu\"],\n",
    "}\n",
    "mlp_GS = GridSearchCV(mlp, param_grid=param_grid, n_jobs=-1, cv=kfold, verbose=False)\n",
    "mlp_GS.fit(X_train, y_train)\n",
    "mlp_GS_best = mlp_GS.best_estimator_\n",
    "print(\"Best score: %0.3f\" % mlp_GS.best_score_)\n",
    "print(\"Best parameters set:\", mlp_GS.best_params_)\n",
    "\n",
    "# save the model\n",
    "filename = \"mlp_GS_best.sav\"\n",
    "pickle.dump(mlp_GS_best, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8. Voting Classifier\n",
    "Voting_ens = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"log\", logreg_CV_best),\n",
    "        (\"rf\", random_forest_CV_best),\n",
    "        (\"knn\", knn_CV_best),\n",
    "        (\"svm\", svr_CV_best),\n",
    "    ],\n",
    "    n_jobs=-1,\n",
    "    voting=\"soft\",\n",
    ")\n",
    "Voting_ens.fit(X_train, y_train)\n",
    "\n",
    "# save the model\n",
    "filename = \"Voting_ens.sav\"\n",
    "pickle.dump(Voting_ens, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}